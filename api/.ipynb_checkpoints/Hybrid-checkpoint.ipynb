{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4 as bs\n",
    "import urllib.request  \n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "DATASET_LOCATION = \"/home/carry/opensoft/OpenSoft-Data/All_FT\"\n",
    "from knapsack import knapsack\n",
    "import heapq \n",
    "from gensim.summarization.summarizer import summarize,_set_graph_edge_weights,_build_graph,_build_corpus,_clean_text_by_sentences, _build_hasheable_corpus\n",
    "from gensim.summarization import keywords\n",
    "from gensim.summarization.pagerank_weighted import pagerank_weighted as _pagerank\n",
    "from gensim.summarization.commons import build_graph as _build_graph\n",
    "from gensim.summarization.commons import remove_unreachable_nodes as _remove_unreachable_nodes\n",
    "case_filenames = [f for f in os.listdir(DATASET_LOCATION) if not f.startswith(\".\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " File : 1978_S_64.txt \n",
      " Summary : It was contended by the learned Counsel for the appellant that the evidence relating to the conduct of the accused when challenged by the Inspector was inadmissible as it was hit by S. 167 Criminal Procedure Code. 5. The defence of the accused was that P.W. 6 met him on 11th July, 1969 and. wanted to make some corrections. On 14th July 1969, P.W. 6 came to his office and wanted the file for making the necessary corrections. Where the circumstances justify it, a Court may refuse to act upon the uncorroborated testimony of a trap witness. \n",
      " Words : 97 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS=100\n",
    "for i,case_filename in enumerate(case_filenames[:-1]):\n",
    "    with open('{}/{}'.format(DATASET_LOCATION,case_filename)) as f:\n",
    "        text = f.read().strip()\n",
    "        text = text.split(\"\\n\",6)[6]\n",
    "#         print(text)\n",
    "    sentences = _clean_text_by_sentences(text)\n",
    "    sent_for_nltk = [sent.text for sent in sentences]\n",
    "    nltk_str = \" \".join(sent_for_nltk)\n",
    "    corpus = _build_corpus(sentences)\n",
    "    hashable_corpus = _build_hasheable_corpus(corpus)\n",
    "    graph = _build_graph(hashable_corpus)\n",
    "    _set_graph_edge_weights(graph)\n",
    "    _remove_unreachable_nodes(graph)\n",
    "    pagerank_scores = _pagerank(graph)\n",
    "    sentences_by_corpus = dict(zip(hashable_corpus, sentences))\n",
    "    get_sentences = [sentences_by_corpus[tuple(doc)] for doc in hashable_corpus[:-1]]\n",
    "    get_scores = [pagerank_scores.get(doc) for doc in hashable_corpus[:-1]]\n",
    "    \n",
    "    word_frequencies = {}  \n",
    "    for word in nltk.word_tokenize(nltk_str):  \n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    sentence_scores = {}  \n",
    "    stopped_sentences = []\n",
    "    sani_sent_list = []\n",
    "    new_sent = sent_for_nltk[0]\n",
    "    for j in range(len(sent_for_nltk)-1):\n",
    "        last_word = new_sent.split(\" \")[-1]\n",
    "        if last_word and last_word[-1] != \".\":\n",
    "            new_sent += \".\"\n",
    "        last_word = last_word[:-1]\n",
    "        if len(last_word) < 4 or \".\" in last_word or \"/\" in last_word:\n",
    "            new_sent += (\" \" + sent_for_nltk[j+1])\n",
    "        else:\n",
    "            sani_sent_list.append(new_sent)\n",
    "            new_sent = sent_for_nltk[j+1]\n",
    "    if new_sent.split(\" \")[-1][-1] != \".\":\n",
    "        new_sent += \".\"\n",
    "    sani_sent_list.append(new_sent)\n",
    "    for sent in sani_sent_list:\n",
    "        j=0\n",
    "        stopped_sent_words = []\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            j=j+1\n",
    "            if word in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]              \n",
    "                stopped_sent_words.append(word)\n",
    "            stopped_sentences.append(\" \".join(stopped_sent_words))\n",
    "        sentence_scores[sent]=sentence_scores[sent]/j     \n",
    "    for j,get_score in enumerate(get_scores):\n",
    "        if get_scores[j] == None:\n",
    "            get_scores[j]=0\n",
    "    j=0\n",
    "    l=0\n",
    "    final_sentence_scores={} \n",
    "    \n",
    "    for sent in sani_sent_list:\n",
    "        j=j+l\n",
    "        l=0\n",
    "        if sent not in final_sentence_scores.keys():\n",
    "                final_sentence_scores[sent]=0\n",
    "        else:\n",
    "            final_sentence_scores[sent]+=sentence_scores[sent]\n",
    "        for sentence in get_sentences[j:-1]:\n",
    "            if sentence.text[-1]!='.':\n",
    "                sentence.text+='.'\n",
    "            if sent.endswith(sentence.text):\n",
    "                final_sentence_scores[sent]+=get_scores[j]\n",
    "                l=l+1\n",
    "                break\n",
    "            final_sentence_scores[sent]+=get_scores[j]\n",
    "            l=l+1\n",
    "    \n",
    "    summary_sentences = heapq.nlargest(30, final_sentence_scores, key=final_sentence_scores.get)\n",
    "    \n",
    "    \n",
    "    size = [len(s.split(\" \")) for s in summary_sentences]\n",
    "    weights = [final_sentence_scores[s]/len(s.split(\" \")) for s in summary_sentences]\n",
    "    sol = knapsack(size, weights).solve(MAX_WORDS)\n",
    "    max_weight, selected_sizes = sol\n",
    "    summary = \" \".join(summary_sentences[s] for s in selected_sizes)\n",
    "    words_in_summary = len(summary.split(\" \"))\n",
    "    print(\"\\n File : {} \\n Summary : {} \\n Words : {} \\n\".format(case_filename,summary,words_in_summary))\n",
    "    q=open(\"/home/carry/opensoft/OpenSoft-Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
